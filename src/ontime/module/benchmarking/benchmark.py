import sys

sys.path.insert(0, "../../..")
import numpy as np
from ontime.core.time_series.time_series import TimeSeries
import pandas as pd
import time
from typing import List, Union, Tuple
import traceback


class Benchmark:
    # a wrapper for datasets used in Benchmark
    class BenchmarkDataset:
        def __init__(
            self,
            training_set: TimeSeries,
            multivariate: bool = None,
            name: str = None,
            test_set: TimeSeries = None,
            target_columns=None,
            *args,
        ):
            self.training_set = training_set
            self.test_set = test_set
            if multivariate is None:
                multivariate = len(training_set.columns) > 1
            self.multivariate = multivariate
            self.name = name
            if target_columns is None and multivariate:
                target_columns = [training_set.columns.tolist()[0]]
            elif not multivariate:
                target_columns = [training_set.columns.tolist()[0]]
            self.target_columns = target_columns

        def get_train_test_split(self, train_proportion=0.7):
            if self.test_set is None:
                return self.training_set.split_before(train_proportion)
            return self.training_set, self.test_set

    # a wrapper containing models used in Benchmark.
    # can be used as is or overridden to adapt for particular models
    class BenchmarkModelHolder:
        def __init__(self, model_class, name: str, arguments_dict: dict = None):
            self.model = model_class
            self.model_instance = None
            self.args = arguments_dict
            self.name = name

        def instantiate(self, train_set: TimeSeries, test_set: TimeSeries, **kwargs):
            self.model_instance = self.model(**self.args, **kwargs)

        def fit(
            self,
            training_set: TimeSeries,
            test_set: TimeSeries,
            target_column=None,
            multivariate=False,
            **kwargs,
        ):
            if multivariate:
                if target_column is None:
                    target_column = training_set.columns.tolist()[0]
                train = training_set.drop_columns(target_column)
                target = training_set[target_column]
                self.model_instance.fit(train, target, **kwargs)
            else:
                self.model_instance.fit(training_set, **kwargs)

        def predict(
            self,
            horizon,
            test_set: TimeSeries,
            target_column=None,
            multivariate=False,
            **kwargs,
        ):
            if multivariate:
                if target_column is None:
                    target_column = test_set.columns.tolist()[0]
                test = test_set.drop_columns(target_column)
                return self.model_instance.predict(test, horizon, **kwargs)
            return self.model_instance.predict(horizon, **kwargs)

        def name(self):
            return self.name

    # wrapper for user-submitted metrics to be used in Benchmark
    class BenchmarkMetric:
        def __init__(self, metric_function, name: str):
            self.metric = metric_function
            self.name = name

        def compute(self, data: TimeSeries, pred: TimeSeries):
            return self.metric(data, pred)

    def __init__(
        self,
        datasets: List[
            Union[TimeSeries, Tuple[TimeSeries, TimeSeries], BenchmarkDataset]
        ] = None,
        models: List[BenchmarkModelHolder] = None,
        metrics: List[BenchmarkMetric] = None,
        train_proportion=0.7,
    ):
        self.train_proportion = train_proportion
        # contains BenchmarkDatasets
        self.datasets = []
        # contains BenchmarkModelHolders
        self.models = []
        # contains BenchmarkMetrics
        self.metrics = []

        # for holding results
        self.metrics_results = []
        self.results = None

        # initialize datasets, models and metrics
        if metrics is not None:
            self.metrics = metrics
        if datasets is not None:
            i = 1
            for d in datasets:
                self.add_dataset(d, str(i))
                i += 1
        if models is not None:
            for m in models:
                self.add_model(m)

    def add_model(self, model):
        if not isinstance(model, Benchmark.BenchmarkModelHolder):
            raise TypeError(
                f"models must implement {Benchmark.BenchmarkModelHolder}, found{type(model)}."
            )
        self.models.append(model)

    def get_models(self):
        return [m.model_class for m in self.models]

    def add_dataset(
        self,
        dataset: Union[TimeSeries, Tuple[TimeSeries, TimeSeries], BenchmarkDataset],
        target_columns=None,
        name=None,
    ):
        if not (
            isinstance(dataset, TimeSeries)
            or isinstance(dataset, Tuple)
            or isinstance(dataset, Benchmark.BenchmarkDataset)
        ):
            raise TypeError(
                f"datasets must be of type {Benchmark.BenchmarkDataset}, {TimeSeries} or {(TimeSeries, TimeSeries)}, "
                f"found type {type(dataset)}"
            )
        multivariate = True
        if (
            (
                isinstance(dataset, Benchmark.BenchmarkDataset)
                and len(dataset.training_set.columns) < 2
            )
            or (isinstance(dataset, TimeSeries) and len(dataset.columns) < 2)
            or (isinstance(dataset, Tuple) and len(dataset[0].columns) < 2)
        ):
            multivariate = False

        if name is None:
            name = str(len(self.datasets))
        if isinstance(dataset, TimeSeries):
            self.datasets.append(
                Benchmark.BenchmarkDataset(
                    dataset, multivariate, name, target_columns=target_columns
                )
            )
        elif isinstance(dataset, Tuple):
            self.datasets.append(
                Benchmark.BenchmarkDataset(
                    dataset[0],
                    multivariate,
                    name,
                    dataset[1],
                    target_columns=target_columns,
                )
            )
        else:  # it's a BenchmarkDataset
            self.datasets.append(dataset)

    def get_datasets(self, metric):
        return [d for d in self.datasets]

    def add_metric(self, metric: BenchmarkMetric):
        self.metrics.append(metric)

    def get_metrics(self):
        return self.metrics

    def run(self, verbose: bool = False, debug: bool = False):
        # TODO: throw error if models or datasets is empty
        self.results = (
            {}
        )  # self.results[model name][dataset name] = {metric name: value, metric name: value,...}
        if verbose:
            print("Starting evaluation...")
        i = 0
        for source_model in self.models:
            self.results[source_model.name] = {}
            if verbose:
                print(f"Evaluation for model {source_model.name}")

            # internal metrics (computed during run time)
            # multivariate/univariate support measures
            nb_mv_run_failed = len([x for x in self.datasets if x.multivariate])
            nb_uv_run_failed = len(self.datasets) - nb_mv_run_failed
            mv0 = len([x for x in self.datasets if x.multivariate])

            # evaluation time!
            for dataset in self.datasets:
                for target in dataset.target_columns:
                    if verbose:
                        print(f"on dataset {dataset.name}, column {target} ")

                    # initialize variables
                    train_set, test_set = dataset.get_train_test_split(
                        self.train_proportion
                    )
                    nb_features = len(train_set.columns)
                    train_size = len(train_set.time_index)
                    test_size = len(test_set.time_index)
                    train_time = 0
                    inference_time = 0
                    # create new model
                    source_model.instantiate(train_set=train_set, test_set=test_set)
                    if verbose:
                        print(f"training... ", end="")
                    train_success = True
                    try:
                        # train
                        start_time = time.time()
                        source_model.fit(
                            train_set,
                            test_set,
                            target_column=target,
                            multivariate=dataset.multivariate,
                        )
                        train_time = time.time() - start_time

                        # test
                        if verbose:
                            print(f"done, took {train_time}\ntesting... ", end="")
                        start_time = time.time()
                        pred = source_model.predict(
                            test_size,
                            test_set,
                            target_column=target,
                            multivariate=dataset.multivariate,
                        )
                        inference_time = time.time() - start_time
                        if verbose:
                            print(f"done, took {inference_time}")

                    except:  # can't train or test on this dataset
                        train_success = False
                        if dataset.multivariate:
                            nb_mv_run_failed -= 1
                        else:
                            nb_uv_run_failed -= 1
                        if verbose:
                            print(f"Couldn't complete training.")
                            if debug:
                                traceback.print_exc()

                    if train_success:
                        # compute metrics
                        self.results[source_model.name][
                            self._c(dataset.name, target)
                        ] = {
                            "nb features": nb_features,
                            "target column": target,
                            "training set size": train_size,
                            "training time": train_time,
                            "test set size": test_size,
                            "testing time": inference_time,
                            "prediction": pred,
                        }
                        # compute user-submitted metrics
                        for metric in self.metrics:
                            try:
                                if verbose:
                                    print(f"{metric.name}: ", end="")
                                    print(f"{test_set.columns} {target}")
                                self.results[source_model.name][
                                    self._c(dataset.name, target)
                                ][metric.name] = metric.compute(
                                    test_set[target][: len(pred)], pred
                                )
                                if verbose:
                                    print(
                                        self.results[source_model.name][
                                            self._c(dataset.name, target)
                                        ][metric.name]
                                    )
                            except:  # can't compute current metric
                                print(f"Couldn't compute {metric.name}")
                                if debug:
                                    traceback.print_exc()

            # back to model-level stats
            supports_uni = None
            if len(self.datasets) - mv0 > 0:  # if we have univariate ds in our batch
                supports_uni = nb_uv_run_failed > 0
            supports_mult = None
            if mv0 > 0:
                supports_mult = nb_mv_run_failed > 0
            self.results[source_model.name][
                "supports univariate"
            ] = Benchmark._bool_to_symbol(supports_uni)
            self.results[source_model.name][
                "supports multivariate"
            ] = Benchmark._bool_to_symbol(supports_mult)
            i += 1

    @staticmethod
    def _bool_to_symbol(b: bool) -> str:
        if b is None:
            return "unknown"
        if b:
            return "✓"
        return "X"

    def get_report(self):
        if self.results is None:
            return "please invoke run_benchmark() to generate report data"
        txt = []

        for model in self.models:
            s = f"Model {model.name}:\n"
            s += f"Supported univariate datasets: {self.results[model.name]['supports univariate']}\n"
            s += f"Supported multivariate datasets: {self.results[model.name]['supports multivariate']}\n"
            for dataset in self.datasets:
                for target in dataset.target_columns:
                    s += f"Dataset {dataset.name}, {target}:\n"
                    if (
                        self._c(dataset.name, target)
                        not in self.results[model.name].keys()
                    ):
                        s += f"{dataset.name}, {target}: couldn't complete training\n"
                    else:
                        for key in self.results[model.name][
                            self._c(dataset.name, target)
                        ].keys():
                            if key == "prediction":
                                continue
                            s += f"{key}: {self.results[model.name][self._c(dataset.name, target)][key]}\n"
            txt.append(s)
        report = "\n\n".join(txt)
        return report

    def get_report_dataframes(self) -> dict:
        # DISCLAIMER: this is still bugged
        # 1 dataframe per model
        # index = datasets
        # columns = metric and details
        col0 = self.datasets[0].training_set.columns.tolist()[0]
        me_columns = self.results[self.models[0].name][
            self._c(self.datasets[0].name, col0)
        ].keys()
        reports = {}
        for model in self.models:
            report_model = pd.DataFrame(columns=me_columns)
            for dataset in self.datasets:
                for target in dataset.target_columns:
                    if self._c(dataset.name, target) in self.results[model.name].keys():
                        report_dict = self.results[model.name][
                            self._c(dataset.name, target)
                        ]
                        del report_dict["prediction"]
                        report_model.loc[dataset.name] = report_dict

            reports[model.name] = report_model
        return reports

    def get_results(self):
        return self.results

    def _c(self, a, b):
        return f"{a} {b}"
