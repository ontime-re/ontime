{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "550c22a8-085f-45f1-9af5-35896f670515",
   "metadata": {},
   "source": [
    "# Module - Benchmarking\n",
    "Ontime provides a Benchmark class that can be used to run a number of prediction models on a number of datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27e27d12-d338-47d3-8fd3-b31ff80ac57c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-26T10:49:59.276079200Z",
     "start_time": "2024-03-26T10:49:56.344109300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import to be able to import python package from src\n",
    "import sys\n",
    "sys.path.insert(0, '../../../../src')\n",
    "\n",
    "from ontime.module.benchmarking.benchmark import Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476a848a-77d7-4efc-840f-665a7cdc1857",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "A Benchmark instance can be initialized with a list of datasets, models and metrics to run through. When invoking run(), it will train and test every dataset on every model, and compute every metric on the predicted data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30689a0a-9ed7-4eca-ab6e-6de1cb1c9e21",
   "metadata": {},
   "source": [
    "### Preparing models\n",
    "Models are wrapped in BenchmarkModelHolders that will instanciate them for each dataset.\n",
    "If a model can't be instanciated and invoked as in BenchmarkModelHolder's implementation, a child class can be written and submitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71c4bc47-8479-46f1-a4c4-7ad5478020f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.models import ARIMA\n",
    "\n",
    "m1 = Benchmark.BenchmarkModelHolder(ARIMA, 'ARIMA', {'p': 12, 'd': 1, 'q': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de0cc64b-36bf-4c34-8dad-7ec66c9e2540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ontime.core.time_series.time_series import TimeSeries\n",
    "from typing import Any\n",
    "from darts.models import BATS\n",
    "\n",
    "# BenchmarkModelHolder child class example (BATS could be submitted as was done with ARIMA, this is for the sake of example)\n",
    "class BATSHolder(Benchmark.BenchmarkModelHolder):\n",
    "    def __init__(self, name: str, arguments_dict: dict = None):\n",
    "        super().__init__(BATS, name, arguments_dict=arguments_dict)\n",
    "        \n",
    "    def instantiate(self, train_set: TimeSeries, test_set: TimeSeries):\n",
    "        self.model_instance = BATS(**self.args)\n",
    "\n",
    "    def fit(self, training_set: TimeSeries, test_set: TimeSeries):\n",
    "        self.model_instance.fit(training_set)\n",
    "\n",
    "    def predict(self, horizon: Any, dataset: TimeSeries) -> Any:\n",
    "        return self.model_instance.predict(horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4a751a9-4688-4a31-b231-320166455795",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = BATSHolder(name='BATS', arguments_dict = {'use_trend': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e2cf8f-69e1-4988-a8d4-c70b06a29019",
   "metadata": {},
   "source": [
    "\n",
    "### Preparing datasets\n",
    "Datasets submitted to a Benchmark must be of type TimeSeries. Pre-wrapping them into a BenchmarkDataset allows to give them a name, give training and test sets, and declare if it's univariate or multivariate. A tuple of timeseries (train set, test set) can also be submitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a1eb452-f598-4e7c-9f7f-88a1185c3c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ontime.module.data.dataset import Dataset\n",
    "from darts.utils.model_selection import train_test_split\n",
    "\n",
    "d1 = Dataset.AirPassengersDataset.load() \n",
    "ausbeer = Dataset.AusBeerDataset.load()\n",
    "d2 = Benchmark.BenchmarkDataset(ausbeer, multivariate = False, name = \"AusBeerDataset\")\n",
    "heartrate = Dataset.HeartRateDataset.load()\n",
    "heartrate_train, heartrate_test = train_test_split(heartrate, test_size = 0.5)\n",
    "d3 = (heartrate_train, heartrate_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd484f03-399d-4e4e-9b7c-2462e6accbb9",
   "metadata": {},
   "source": [
    "### Preparing metrics\n",
    "Metrics must be wrapped in a Benchmark.BenchmarkMetric instance. Again, if the function can't be invoked as in BenchmarkMetric's implementation, a child class can be written and submitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bd6e05b-d86a-484a-bd28-7559b640c20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import darts\n",
    "\n",
    "me1 = Benchmark.BenchmarkMetric(name=\"RMSE\", metric_function=darts.metrics.metrics.coefficient_of_variation)\n",
    "me2 = Benchmark.BenchmarkMetric(name=\"MAE\", metric_function=darts.metrics.metrics.mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ce9193-2bd1-4859-b5c5-17a48cb4e0fa",
   "metadata": {},
   "source": [
    "## Creating a Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b99abb03-c3ee-4e19-87a7-c86e1b418a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = Benchmark(datasets = [d1, d2, d3], # datasets submitted as simple TimeSeries will be given a number as a name\n",
    "                      models = [m1, m2], \n",
    "                      metrics = [me1, me2], \n",
    "                      train_proportion=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b7a83a-e878-4fbf-818a-3c70f73bcb05",
   "metadata": {},
   "source": [
    "Datasets, models and metrics can also be added after instanciation. This allows to name datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "005d3bae-1fdb-4edd-91ba-0df918fae5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark2 = Benchmark()\n",
    "benchmark2.add_model(m1)\n",
    "benchmark2.add_dataset(d1, name = \"AirPassengerDataset\")\n",
    "benchmark2.add_metric(me1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86ec250-db58-4c10-b4e3-14c2b23bcf70",
   "metadata": {},
   "source": [
    "Once the models and datasets have been added, the run() method will train instances of all the models on all the datasets individually and compute metrics. The verbose parameter will print the status and results of the process as it progresses, and the debug parameter will print error messages (warnings are printed anyways)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e96d8811-efd0-4cb0-bf7c-a03364e05911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation...\n",
      "Evaluation for model ARIMA\n",
      "on dataset 1 \n",
      "training... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charlie/.local/lib/python3.10/site-packages/statsmodels/tsa/statespace/sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-stationary starting autoregressive parameters'\n",
      "/home/charlie/.local/lib/python3.10/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done, took 0.7708535194396973\n",
      "testing... done, took 0.004489421844482422\n",
      "RMSE: 3.6823375607771083\n",
      "MAE: 13.397485829577487\n",
      "on dataset AusBeerDataset \n",
      "training... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charlie/.local/lib/python3.10/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done, took 0.8976242542266846\n",
      "testing... done, took 0.004706382751464844\n",
      "RMSE: 4.350713610378453\n",
      "MAE: 15.369339480209591\n",
      "on dataset 3 \n",
      "training... done, took 3.46706223487854\n",
      "testing... done, took 0.030436277389526367\n",
      "RMSE: 6.606337568439427\n",
      "MAE: 5.044218953626089\n",
      "Evaluation for model BATS\n",
      "on dataset 1 \n",
      "training... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charlie/.local/lib/python3.10/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done, took 7.672419548034668\n",
      "testing... done, took 0.0022764205932617188\n",
      "RMSE: 8.321211346269285\n",
      "MAE: 33.570078021466806\n",
      "on dataset AusBeerDataset \n",
      "training... done, took 12.87717056274414\n",
      "testing... done, took 0.0022611618041992188\n",
      "RMSE: 4.378445630437863\n",
      "MAE: 13.398303193058819\n",
      "on dataset 3 \n",
      "training... done, took 16.338427543640137\n",
      "testing... done, took 0.002260923385620117\n",
      "RMSE: 6.577731759671614\n",
      "MAE: 4.963032869193333\n"
     ]
    }
   ],
   "source": [
    "benchmark.run(verbose = True, debug = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4038313-7cf4-480d-8d3c-fd030e100da3",
   "metadata": {},
   "source": [
    "To view the results, you can call get_report() and print the returned value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35b9b732-bd80-426f-a8bb-134f06bf1a3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ARIMA:\n",
      "Supported univariate datasets: ✓\n",
      "Supported multivariate datasets: unknown\n",
      "Dataset 1:\n",
      "nb features: 1\n",
      "training set size: 128\n",
      "training time: 0.7708535194396973\n",
      "test set size: 16\n",
      "testing time: 0.004489421844482422\n",
      "RMSE: 3.6823375607771083\n",
      "MAE: 13.397485829577487\n",
      "Dataset AusBeerDataset:\n",
      "nb features: 1\n",
      "training set size: 189\n",
      "training time: 0.8976242542266846\n",
      "test set size: 22\n",
      "testing time: 0.004706382751464844\n",
      "RMSE: 4.350713610378453\n",
      "MAE: 15.369339480209591\n",
      "Dataset 3:\n",
      "nb features: 1\n",
      "training set size: 900\n",
      "training time: 3.46706223487854\n",
      "test set size: 900\n",
      "testing time: 0.030436277389526367\n",
      "RMSE: 6.606337568439427\n",
      "MAE: 5.044218953626089\n",
      "\n",
      "\n",
      "Model BATS:\n",
      "Supported univariate datasets: ✓\n",
      "Supported multivariate datasets: unknown\n",
      "Dataset 1:\n",
      "nb features: 1\n",
      "training set size: 128\n",
      "training time: 7.672419548034668\n",
      "test set size: 16\n",
      "testing time: 0.0022764205932617188\n",
      "RMSE: 8.321211346269285\n",
      "MAE: 33.570078021466806\n",
      "Dataset AusBeerDataset:\n",
      "nb features: 1\n",
      "training set size: 189\n",
      "training time: 12.87717056274414\n",
      "test set size: 22\n",
      "testing time: 0.0022611618041992188\n",
      "RMSE: 4.378445630437863\n",
      "MAE: 13.398303193058819\n",
      "Dataset 3:\n",
      "nb features: 1\n",
      "training set size: 900\n",
      "training time: 16.338427543640137\n",
      "test set size: 900\n",
      "testing time: 0.002260923385620117\n",
      "RMSE: 6.577731759671614\n",
      "MAE: 4.963032869193333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(benchmark.get_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0852085-874e-4583-9f91-1efcb486fbcb",
   "metadata": {},
   "source": [
    "You can also get results by calling get_report_dataframes(). The results are then returned as a dictionary with the model names as keys and dataframes as values, where columns are measures (testing time, metrics, etc.) and index is the dataset names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "687a0b79-ff02-4bde-aff7-6d6ccd5e7d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA:\n",
      "               nb features training set size training time test set size  \\\n",
      "1                      1.0             128.0      0.770854          16.0   \n",
      "AusBeerDataset         1.0             189.0      0.897624          22.0   \n",
      "3                      1.0             900.0      3.467062         900.0   \n",
      "\n",
      "               testing time      RMSE        MAE  \n",
      "1                  0.004489  3.682338  13.397486  \n",
      "AusBeerDataset     0.004706  4.350714  15.369339  \n",
      "3                  0.030436  6.606338   5.044219  \n",
      "\n",
      "BATS:\n",
      "               nb features training set size training time test set size  \\\n",
      "1                      1.0             128.0       7.67242          16.0   \n",
      "AusBeerDataset         1.0             189.0     12.877171          22.0   \n",
      "3                      1.0             900.0     16.338428         900.0   \n",
      "\n",
      "               testing time      RMSE        MAE  \n",
      "1                  0.002276  8.321211  33.570078  \n",
      "AusBeerDataset     0.002261  4.378446  13.398303  \n",
      "3                  0.002261  6.577732   4.963033  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs = benchmark.get_report_dataframes()\n",
    "for df in dfs.keys():\n",
    "    print(f'{df}:')\n",
    "    print(dfs[df])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a113a1a8-6988-4440-9afc-c4e9b5e6f898",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
